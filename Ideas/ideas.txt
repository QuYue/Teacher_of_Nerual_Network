方案：
一、无监督：（不太靠谱）
1.  综合接近0：认为经过z-score标准化后，某个样本的某个特征的值越接近0则所包含信息量越少，那么可以取样本的所有特征之和，越小则所包含总信息量越少，那么认为这个样本越简单，容易学习，则可以早些前学，后面学习难的样本。
2. 0项多的：同上面的理论，但是只考虑接近0的特征的数量做为信息量
3. 题型汇总：找类似的样本（可用欧式距离或汉明距离）当做一种题型，可以做聚类，然后一个batch输入一种题型，集中式学习，考点逐一突破。
4. 题型汇总：同上的理念，但是一个题型多次输入，然后可以利用遗忘曲线的方式日后回顾。
5. 遗忘曲线：利用遗忘曲线，随机给出样本的复习时间。（固定样本次数）
6. 控制变量：找到可以控制变量的样本，一次batch进行输入，（即汉明距离为1，最好包含了不同的分类结果）

二、有监督：（感觉更靠谱一点，类似模糊抽样）
1. 错题回顾：找到错题，多次回顾，可以结合遗忘曲线，也可以强行放入到下个batch中训练，但是都要设一个最大次数（防止是错题）
2. 错题回顾+考题汇总：同上面的理念，找到错题，然后不单单是复习一道题，还要找到同类的题（欧式距离或汉明距离）进行复习，当然也可以结合遗忘曲线
3. 错题回顾+控制变量：同上面的理念，找到错题，然后不单单是复习一道题，还要进行控制变量进行复习，当然也可以结合遗忘曲线

三、强化学习方案：（感觉最靠谱）
1. 误差学习：通过强化学习，将误差（train_data全部）输入到强化学习的状态中作为反馈，然后奖赏是达到90%的学习的总时间，或总迭代次数（或其他），每次对样本进行选择是状态。（当然如果能将误差不使用更好）
2. 特征+误差(有点像embedding)：同上，但是样本的特征也要输入。

四、Embedding方案：（做出来会很牛逼，如果能用数学证明会更牛逼，和强化学习结合是一个很好的方案，可以深度挖掘出样本的信息量）
1. 进行embedding，找到最好的选择方案

信息熵，或概率？
dropout